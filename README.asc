= dups

*dups* is the duplicate files finder. The aim is to keep it simple.

== Algorithm

In the first step, *dups* scans requested directories and builds a
tree of file sets consisting of files that have the same size. Inside a
file set files are sorted by device number and inode. Sorting by inode
usually makes I/O operations faster (look into "Linux System Programming"
by Robert Love, Chapter 4 for more about it).

For each file set, all files are opened simultaneously and read one
block at a time. Files with different block contents are separated into
smaller filesets. The process continues until files are unique in their
sets or end of file is reached. If end of file is reached for some subset
of files, all of them are marked as duplicates.

The algorithm is not the fastest. For example, `rdfind' usually
outperforms `dups'. Faster tools rely mostly on some combination of block
comparison and hashing and use heuristics like reading blocks from the
end of files.

== Benchmarking

When benchmarkng and comparing performance to other
tools, do not forget to drop filesystem cache:
https://www.linuxquestions.org/questions/linux-kernel-70/how-to-disable-filesystem-cache-627012/

Run dups on benchmark directory:
----
# sync && echo 3 >/proc/sys/vm/drop_caches && time ./dups benchmark/ >/dev/null
----

Also, check read(2) syscall times with `strace -c'.

== Alternatives
- ddff: http://yurichev.com/ddff.html
- duff: http://duff.dreda.org/
- dupseek: http://www.beautylabs.net/software/dupseek.html
- fastdup: https://github.com/rburchell/fastdup, previously https://sourceforge.net/projects/fastdup/
- fdups: https://sourceforge.net/projects/fdups/
- freedup: http://freedup.org/
- fslint: http://www.pixelbeat.org/fslint/
- merge_dups: http://www.asheesh.org/note/software/duplicate-files.html
- rdfind: https://rdfind.pauldreik.se/
- rmlint: https://github.com/sahib/rmlint
